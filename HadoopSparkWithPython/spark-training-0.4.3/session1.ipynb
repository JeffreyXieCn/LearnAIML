{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background:black\">\n",
    "    <center>\n",
    "<img src=\"./images/session1/title.png\" alt=\"Title\"/>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<center>At the end of this session, you will know:</center>\n",
    "\n",
    "&#x25a2; _when_ to use Spark\n",
    "\n",
    "&#x25a2; _how_ to write Spark programs\n",
    "\n",
    "&#x25a2; what is the relation between Spark and _Hadoop_\n",
    "</div>\n",
    "\n",
    "<p style=\"float: left; font-size: 9pt; text-align: center; width: 30%; margin-right: 30%; margin-bottom: 0.5em;\"><a href=\"http://spark.apache.org\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" width=150></a></p>\n",
    "<p style=\"float: left; font-size: 9pt; text-align: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\"><a href=\"http://hadoop.apache.org\"><img src=\"https://hadoop.apache.org/hadoop-logo.jpg\" width=290></a></p>\n",
    "<p style=\"clear: both;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Goal:</b> process large datasets easily and efficiently \n",
    "</div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key concept 1: Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<center>Apache Spark is built around a data structure: the <b>Resilient Distributed Dataset (RDD)</b></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "## Why distributed?\n",
    "\n",
    "<a href=\"https://www.google.com/about/datacenters/gallery/\">\n",
    "<img src=\"https://lh3.googleusercontent.com/_T5y2eKUusOWBn44MkgTDc1EQVsiGkvWXDDgbNZxeKOp1aHKYpIMS56JhU3esg6F_V6sbmGmxmThuk5ugETygfPdv2ssbVRjHD3fcw=w800-l80-sg-rj-c0xffffff\" alt=\"google data center\"/></a>\n",
    "\n",
    "Photo (c) Google\n",
    "    \n",
    "```\n",
    "Our Council Bluffs, Iowa data center provides over 115,000 square feet of space. We make the best out of every inch, so you can use services like Search and YouTube in the most efficient way possible.\n",
    "```\n",
    "   </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "    <b>Cluster architecture</b>\n",
    "    <img src=\"./images/session1/cluster-architecture.png\" alt=\"cluster architecture\"/>\n",
    "    adapted from <a href=\"htttp://mmds.org\">http://mmds.org</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<center>An RDD can be distributed among many cluster nodes, aggregating CPU, memory and disk resources.</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "## Why resilient?\n",
    "\n",
    "<a href=\"https://www.google.com/about/datacenters/gallery/\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/0N2MQjMcGLlmHE4p-0FRdsKUyKieC-ga0UtW5HVDB1HHBbmiRj2kA2TCDgyWaK2ZzvrKpWoI4c1djmRi0I-ujnNCo43qpQp7e4q3TA=w800-l80-sg-rj-c0xffffff\" alt=\"data center 1\"/>\n",
    " </a>\n",
    "\n",
    "Photo (c) Google\n",
    "    \n",
    "```\n",
    "Blue LEDs on this row of servers in our Douglas County, Georgia data center tell us everything is running smoothly. We use LEDs because they are energy efficient, long lasting and bright.\n",
    "```\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <b>The Hadoop Distributed File System (HDFS)</b>\n",
    "<img src=\"images/session1/hdfs.png\"/>\n",
    "    adapted from <a href=\"htttp://mmds.org\">http://mmds.org</a>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <center>An RDD can be stored on HDFS, where data chunks are <b>replicated</b> across nodes.</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <b>File reads</b>\n",
    "<img src=\"images/session1/hdfs2.png\"/>\n",
    "    Source: Hadoop: The Definitive Guide, Tom White, 4th edition.\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <b>File writes</b>\n",
    "<img src=\"images/session1/hdfs1.png\"/>\n",
    "    Source: Hadoop: The Definitive Guide, Tom White, 4th edition.\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>An RDD can also be <b>recomputed</b>, as it stores its own lineage.</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/session1/lineage.png\"/>\n",
    "<a href=\"https://dl.acm.org/doi/fullHtml/10.1145/2934664\">[Zaharia <i>et al.</i>, 2016]</a>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating RDDs\n",
    "\n",
    "Before we can create and manipulate RDDs, let's make sure that Apache Spark is properly installed. Spark comes with APIs in four languages: Java, Scala, R and Python. Here we will use the Python API.\n",
    "\n",
    "`pyspark` is the Python package that provides Spark's Python API. Make sure that version 3.0.0 is properly installed on your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! pip freeze | grep pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A little magic to adjust the config at Ericsson\n",
    "import os\n",
    "os.environ[\"IPYTHON\"]=\"1\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"ipython3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/default-java\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Spark, many functions are provided through a helper object called the Spark context. A Spark context can be created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark context\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RDDs can be created in three ways:\n",
    "\n",
    "(1) From an existing Python list, using function <code>parallelize</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create \"numbers\", an RDD defined from a Python list\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The content of an RDD can be inspected using <b>actions</b> such as `collect`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the RDD back to a Python list, for inspection\n",
    "numbers.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Warning:</b> actions can be very dangerous!\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(2) RDDs can also be created from a file, using `textFile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read a text file in an RDD where each element is a line of the file\n",
    "words = sc.textFile('data/session1/words.txt')\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>: <code>textFile</code> can also read data from HDFS\n",
    "        </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(3) Or from an existing RDD, using a <b>transformation</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a random sample with 50% of the data\n",
    "sample = numbers.sample(fraction=0.5, withReplacement=False)\n",
    "sample.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manipulating RDDs\n",
    "\n",
    "RDDs are manipulated using <b>transformations</b> and <b>actions</b>:\n",
    "\n",
    "<a href=\"http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\"><img src=\"images/session1/transformations-actions.png\" alt=\"Transformations and actions\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Several transformations, such as `filter`, take a function as argument. This function can be either explicitly defined, or defined in the argument of the transformation as an anonymous (aka \"lambda\") function.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function definition\n",
    "def even(x):\n",
    "    '''\n",
    "    Return True if x is an even integer\n",
    "    '''\n",
    "    return x % 2 == 0\n",
    "\n",
    "# Extracts the even numbers from RDD \"numbers\"\n",
    "even = numbers.filter(even)\n",
    "\n",
    "# Note: each partition of an RDD can be processed independently using filter\n",
    "\n",
    "even.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Same operation, now with the function declared as a lambda function\n",
    "even = numbers.filter(lambda x: x % 2 == 0)\n",
    "even.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "<li>Create an RDD that contains the words in <code>words</code> that have four letters or more.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use-case (part 1): Open Data from the City of Montreal\n",
    "\n",
    "We will study <a href=\"http://donnees.ville.montreal.qc.ca/dataset/frenes-publics-proteges-injection-agrile-du-frene \">this dataset</a>, provided by the city of Montreal. \n",
    "\n",
    "The dataset contains \n",
    "the list of trees treated against the <a href=\"https://en.wikipedia.org/wiki/Emerald_ash_borer\">emerald ash borer</a> in Montreal.\n",
    "\n",
    "We will use the 2015 and 2016 data sets also available in directory `data` made available with this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>Our goal is to use Spark to extract basic statistics about the dataset.</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As CSV files, these datasets can be read by many libraries, such as `Pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/session1/frenepublicinjection2015.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <center><b>Note:</b> We could use Pandas instead of Spark to analyze this dataset, but it wouldn't scale to larger datasets.</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To start with, let's load this dataset in Spark, using Python's CSV module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename2015 = 'data/session1/frenepublicinjection2015.csv'\n",
    "\n",
    "def read_data_file(filename):\n",
    "    import csv\n",
    "    # Load the text file as an RDD, where each element is a line\n",
    "    rdd = sc.textFile(filename)\n",
    "    # Use Python's CSV reader to map every line of the text file to a Python list\n",
    "    trees = rdd.mapPartitions(lambda x: csv.reader(x))\n",
    "    return trees\n",
    "    \n",
    "# Show the first two elements of the RDD, with action \"take\"\n",
    "read_data_file(filename2015).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Print the number of trees that were treated in 2015 and 2016. Tip: use the <code>count</code> action.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Print the number of trees in park <code>BEAUBIEN</code> that were treated in 2015. Tip: use the <code>filter</code> action on column <code>Nom_parc</code> (index: 6)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "A Spark implementation of a data analysis will always be faster than a Pandas implementation of the same analysis:\n",
    "    \n",
    "    \n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Writing a Spark program requires detailed knowledge of cluster computing:\n",
    "    \n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Spark analyses are limited to basic data queries:  \n",
    "    \n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key concept 2: MapReduce\n",
    "\n",
    "\n",
    "MapReduce is arguably the most famous and powerful <b>programming model</b> for Big Data analysis. \n",
    "\n",
    "Introduced by <a href=\"https://dl.acm.org/doi/abs/10.1145/1327452.1327492?casa_token=DCf_01yMfcQAAAAA:lOY3pLjhcDj4YPtDqowe7zcsZd3bpqgqZW8x0KYaadjEPFas7Id1O4g4t6idEAVUeS2ebaeyFumGkw\">Google in 2008</a>, it is available in Spark through the `map` and `reduceByKey` transformations.\n",
    "\n",
    "## Map\n",
    "\n",
    "`map` is a transformation that evaluates a function on each element of an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "upper = words.map(lambda x: x.upper())\n",
    "upper.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Create an RDD that contains the square roots of the numbers in RDD <code>numbers</code>.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduce\n",
    "\n",
    "A reduce operation works on __key-value pairs__.\n",
    "\n",
    "Transformation `groupByKey` groups key-value pairs by key.\n",
    "\n",
    "For instance, when applied to:\n",
    "\n",
    "`[ ('apple', 3), ('banana', 2), ('apple', 7), ('banana', 5) ]`, \n",
    "\n",
    "`groupByKey` returns:\n",
    "\n",
    "`[ ('apple', [ 3, 7 ]), ('banana', [2, 5]) ]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create RDD\n",
    "fruits = [ ('apple', 3), ('banana', 2), ('apple', 7), ('banana', 5) ]\n",
    "fruits_rdd = sc.parallelize(fruits)\n",
    "\n",
    "# Apply groupByKey\n",
    "group = fruits_rdd.groupByKey()\n",
    "\n",
    "# Map values to list to display\n",
    "group.mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark transformation `reduceByKey` groups key-value pairs by key, and applies a binary aggregation function to the list of values associated with a key.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fruits_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Add all the values associated with a key\n",
    "fruits_rdd.reduceByKey(lambda x, y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "MapReduce is an extremely <b>versatile</b> programming model: any distributed program can be implemented using map and reduce operations.\n",
    "</div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "WordCount is the historical MapReduce example, aiming at counting the occurrence of words in a document. For instance, if the document contains ten occurrences of the word \"house\", the result returned by WordCount will contain (\"house\", 10).\n",
    "    </div>\n",
    "    </center>\n",
    "    \n",
    "The MapReduce WordCount implementation is based on the following principle:\n",
    "1. Create an RDD where each element is a word in the input document\n",
    "2. Use the `map` transformation to create an RDD containing `(word, 1)` for every word in the document\n",
    "3. Use the `reduceByKey` action to sum the 1s associated with a given word.\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "<li>Implement a WordCount program in Spark to count the occurence of words in file <code>data/places.txt</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise (optional): Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    An <b>inverted index</b> is a data structure that, given a set of text documents, returns for each word the set of documents in which it occurs. It is the basic structure used in a <b>Web search engine</b>, and a key historical motivation for Google to develop MapReduce.\n",
    "    </div>\n",
    "    </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assume that an RDD <code>docs_and_words</code> exists that contains pairs <code>doc,word</code> where <code>doc</code> identifies a document and <code>word</code> is a word contained in this document. The following code generates such an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Countries for which a document exists in data/sesssion1/docs\n",
    "countries = ('canada', 'usa', 'mexico')\n",
    "# Initialize an empty RDD.\n",
    "docs_and_words = sc.emptyRDD()\n",
    "# Iterate over documents\n",
    "for country in countries:\n",
    "    file = f'data/session1/docs/{country}.txt'\n",
    "    \n",
    "    # Get all the words in the document, in the form of (doc,word) pairs\n",
    "    words = sc.textFile(file)\\\n",
    "              .flatMap(lambda x: x.split(' '))\\\n",
    "              .map(lambda x: (country+'.txt', x.lower()))\n",
    "    \n",
    "    # Add to the list of docs_and_words\n",
    "    docs_and_words = docs_and_words.union(words)\n",
    "# Check result\n",
    "docs_and_words.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Transform <code>docs_and_words</code> into an inverted index containing pairs of the form <code>word,[docs]</code>, where <code>[docs]</code> is the list of documents that contain <code>words</code>. Sort the inverted index by key (word). Tip: to remove duplicates in a Python list, use a <a href=\"https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset\"><code>set</code></a> instead.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use case (part 2)\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "<li>Print the list of unique parks where trees were treated, ordered alphabetically. Tip: use the <code>distinct</code> and <code>sortBy</code> transformations described in the <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">Spark documentation</a>.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Create an RDD containing elements in the form <code>park,count</code>, where <code>count</code> is the number of trees treated in park <code>park</code>. Tip: use a logic similar to WordCount's.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Optional__\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "<li>Print the list of the 10 parks with the highest number of treated trees, ordered by decreasing count of treated trees. Tip: start from the solution to the previous question.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " __Optional__\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>\n",
    " Print the alphabetically sorted list\n",
    "        of parks that had trees treated both in 2016 and 2015. Tip: use the <code>intersection</code> transformation.\n",
    "    </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The MapReduce programming model was first introduced in Apache Spark:  \n",
    "    \n",
    "    \n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The map transformation is parallel: provided that enough computers are available in the cluster, partitions of an RDD will be processed concurrently by the map function:\n",
    "\n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The reduce transformation may result in important overheads due to data transfers:  \n",
    "    \n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Spark is limited to the processing of textual data. Binary data types, such as signals or images, are not supported:\n",
    "    \n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key concept 3: Data locality, Lazy evaluation, In-memory computing\n",
    "\n",
    "## Data locality\n",
    "\n",
    "Data transfers can be an important source of overhead in a distributed environment. \n",
    "\n",
    "To limit data movement, Spark:\n",
    "- schedules tasks preferably to the nodes where input data is located (HDFS helps)\n",
    "- leaves output data on the nodes where it was produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <b>The effect of data locality <a href=\"https://arxiv.org/pdf/1812.06492\">[Hayot-Sasson <i>et al.</i>, 2019]</a></b>\n",
    "<img src=\"images/session1/data-locality.png\"/>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Impact of data locality is maximal for <b>data-intensive</b> applications\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "When an action is called, Spark combines transformations to avoid unnecessary computations. \n",
    "    </div>\n",
    "\n",
    "\n",
    "<img src=\"images/session1/lazy.png\" alt=\"lazy evaluation\" width=800/>\n",
    "(diagram by Val√©rie Hayot-Sasson)\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-memory computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Spark keeps data in memory as much as possible, which makes it faster than Hadoop MapReduce\n",
    "    </div>\n",
    "\n",
    "    \n",
    "<img src=\"images/session1/memory.png\" alt=\"in memory benchmark\"/>\n",
    "\n",
    "<a href=\"https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia\">[Zaharia <i>et al.</i>, 2012]</a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key concept 4: DataFrames\n",
    "\n",
    "## Creating and manipulating DataFrames\n",
    "\n",
    "\n",
    "Similar to R or Pandas, Spark can also manipulate data through DataFrames representing textual tables. \n",
    "\n",
    "The DataFrame API requires to initialize a Spark session from Spark's SQL API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The DataFrame API is particularly suited for the processing of CSV files, as is the case of our use case. CSV files can be loaded as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'data/session1/frenepublicinjection2015.csv'\n",
    "df = spark.read.option(\"header\",\"true\").csv(filename)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pandas data frames are more nicely rendered in Jupyter than Spark's, so for the sake of visualization we can convert our Spark DataFrame to Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark DataFrames leverage RDDs. In fact, the internal representation of a DataFrame is an RDD, which can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Several functions are available to process DataFrames, documented [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). \n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Counting\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering\n",
    "df.filter(df['Nom_parc'] == 'BEAUBIEN').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use case (part 3): with DataFrames\n",
    "\n",
    "We will now repeat the analyses done on our dataset, this time with the DataFrame API.\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "<li>Using the DataFrame API, print the list of unique parks where trees were treated, ordered alphabetically. Tip: use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select\"><code>select</code></a> function.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Using the DataFrame API, print a list of elements in the form <code>park,count</code>, where <code>count</code> is the number of trees treated in park <code>park</code>. Tip: use function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\"><code>groupBy</code></a>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Optional__\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "<li>Print the list of the 10 parks with the highest number of treated trees, ordered by decreasing count of treated trees. Tip: start from the solution to the previous question.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " __Optional__\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>\n",
    " Print the alphabetically sorted list\n",
    "    of parks that had trees treated both in 2016 and 2015. Tip: use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\"><code>join</code></a> function.\n",
    "    </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    &#x2611; <i>when</i> to use Spark\n",
    "</div>\n",
    "\n",
    "<i>Large datasets, substantial processing, independent tasks, batch processing</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    &#x2611; <i>how</i> to write Spark programs\n",
    "</div>\n",
    "\n",
    "<i>Two APIs: Resilient Distributed Datasets, DataFrames</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    &#x2611; <i>what</i> is the relation between Spark and <i>Hadoop</i>\n",
    "</div>\n",
    "\n",
    "<i>Hadoop MapReduce was the first public implementation of MapReduce. Spark now outperforms it and provides a simpler API.</i>\n",
    "\n",
    "<i>The Hadoop Distributed File System (HDFS) can be combined with Spark to improve data locality.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <b>The Apache Spark ecosystem <a href=\"https://dl.acm.org/doi/fullHtml/10.1145/2934664\">[Zaharia <i>et al.</i>, 2016]</a></b>\n",
    "<img src=\"images/session1/ecosystem.png\" alt=\"ecosystem\" width=600/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "360.717px",
    "left": "1528.05px",
    "top": "33.1333px",
    "width": "369.95px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
