{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background:black\">\n",
    "    <center>\n",
    "<img src=\"./images/session4/title.png\" alt=\"Title\"/>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>\n",
    "Today's objectives<br/><br/>\n",
    "    </center>\n",
    "    &#x25a2; Practice with a <b>real cluster</b><br/><br/>\n",
    "    &#x25a2; Get familiar with the concept of <b>data streams</b><br/>\n",
    "    &#x25a2; Practice with Spark's <b>DStream</b> API<br/>\n",
    "    &#x25a2; Analyze a data stream for <b>Human Activity Recognition</b> in the Internet of Things\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cluster deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    An <b>HDFS</b> and <b>YARN</b> cluster was deployed by the Ericsson support team. We will use it to practice with HDFS and with Spark applications.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More on HDFS\n",
    "\n",
    "See slides <a href=\"pdf/session4/HDFS_intro.pdf\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDFS examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "HDFS is accessible using command <code>hdfs dfs</code>\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Open a terminal and skim through the output of <code>hdfs dfs --help</code></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given that the file system is available at <code>hdfs://10.0.2.8:9000</code> (use this URL with option <code>--fs</code> of <code>hdfs df</code>):\n",
    "            \n",
    "            \n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>List the root directory (<code>/</code>) of the file system</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Create a directory named after your name under <code>/user</code></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Create a text file and upload it to your directory</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Using the <code>cat</code> command, display the content of the text file as stored on the HDFS</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Download the text file again, and check its content</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Remove the text file from HDFS</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Using command <code>hdfs fsck</code>, inspect the status of the files under your directory</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "HDFS data nodes are also compute nodes:<br/>\n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "HDFS data nodes are also compute nodes:<br/>\n",
    "&#x2611; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>\n",
    "This is a key feature to ensure data locality (see Session 1, Key concept 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "HDFS allows clusters to scale horizontally, which is very cost-efficient:<br/>\n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2;  False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "HDFS allows clusters to scale horizontally, which is very cost-efficient:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x2611; False\n",
    "</div>\n",
    "\n",
    "Since all cluster nodes are both compute and storage nodes in an HDFS cluster, adding more nodes to the cluster increases the overall storage and compute capacity of the infrastucture. This is called <b>horizontal scaling</b>. Horizontal scaling in HDFS is very cost efficient to adjust the infrastructure to data-intensive applications of various sizes. However, HDFS wouldn't be a cost-effective model when the cluster needs to accomodate a <b>mix of data-intensive and compute-intensive applications</b>. In this case, storage available on compute nodes would remain unused when compte-intensive applications are run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is important for data stored on HDFS to remain balanced among data nodes:<br/>\n",
    "&#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is important for data stored on HDFS to remain balanced among data nodes:<br/>\n",
    "&#x2611; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>\n",
    "An imbalanced cluster is detrimental to both performance and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running Spark on a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See slides [here](https://docs.google.com/presentation/d/1ub1Rlpe-18RZ_f432k8wx9TB-8YYl8DOrZRcIzNp9pA/edit#slide=id.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark jobs can be submitted to a cluster using command <code>spark-submit</code>\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Open a terminal and skim through the output of <code>spark-submit --help</code></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "File <code>test_cluster/spark_test.py</code> contains a simple Spark program.\n",
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Run this script with <code>spark-submit</code>, without using any option. The default deploy mode is standalone (the program runs locally).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Submit this script to the Spark cluster, keeping in mind that:\n",
    "    <ol>\n",
    "        <li>The scheduler is YARN</li>\n",
    "        <li>The deply mode must be <code>cluster</code>\n",
    "        <li>You will need to use the following option in <code>spark-submit</code>: <code>--conf spark.yarn.stagingDir=hdfs://10.0.2.8:9000/user</code>\n",
    "        </ol> \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "   Spark can also read files from HDFS\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "File <code>session1_cluster.py</code> contains a Spark program extracted from the use case in session 1. It takes a single argument, the tree file to process (for instance <code>data/session1/frenepublicinjection2015.csv</code>).\n",
    "    \n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Run this Spark program <b>locally</b>, on a <b>local file</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "        <li>Upload the input file to HDFS and run the Spark program <b>locally</b>, on the <b>HDFS file</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "        <li>Run the Spark program <b>on the cluster</b>, on the <b>HDFS file</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to data streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See slides [here](https://docs.google.com/presentation/d/1NtjwD80gHwuCyloC4TmH-_tZyCumz52Ise_jrRWPG5s/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark data stream APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   Spark has two APIs to handle data streams: <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\"><b>Structured Streaming</b></a> (DataFrames) and <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html\"><b>Discretized Streams</b></a> (RDDs).\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretized Streams (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DStreams work by splitting input data into batches, represented as RDDs:\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/streaming-flow.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "   Batches can be processed using Spark's core RDD API:\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\" alt=\"stream-rdd\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In real life, streams originate in sources such as:\n",
    "- Twitter\n",
    "- [Apache Kafka](http://kafka.apache.org)\n",
    "- [Apache Flume](https://flume.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data stream sources in Spark:\n",
    "<ul>\n",
    "    <li> Files: <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html?highlight=textfilestream#pyspark.streaming.StreamingContext.textFileStream\"><code>textFileStream</code></a></li>\n",
    "    <li>Network: <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html?highlight=textfilestream#pyspark.streaming.StreamingContext.socketTextStream\"><code>socketTextStream</code></a></li>\n",
    "    <li>A list of RDDS: <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.queueStream\"><code>queueStream</code></a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    In this session we will use <code>queueStream</code>, to simplify deployment\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A stream is created from a dedicated stream context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext.getOrCreate() \n",
    "ssc = StreamingContext(sc, 1) # Batches will be pulled every second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this example, we will implement classical WordCount. Our stream will be simulated from a list of RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdds = [ sc.parallelize([ \"one\", \"two\", \"three\"]),\n",
    "           sc.parallelize([ \"one\", \"three\", \"four\"]),\n",
    "           sc.parallelize([\"two\", \"five\", \"six\"])\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The stream is defined using <code>queueStream</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stream = ssc.queueStream(rdds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many transformations available on RDDs can be directly applied to DStreams, as documented [here](https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams). Here we implement a simple MapReduce task on the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pairs = stream.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "counts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "counts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Until now, processing hasn't started. The stream will start receiving data when the `start` function is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `stop` function terminates the processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: counting unique elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Goal</b>: implement the Flajolet-Martin algorithm to count unique elements in a stream\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Step 1</b>: counters and estimation\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will create $n$ global counters to store the max number of trailing zeros in the binarized output of each hash function.\n",
    "\n",
    "In Spark, global counters are called <b>accumulators</b>. Thy can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def get_counter(n):\n",
    "    counter_name = \"counter-\"+str(n)\n",
    "    if (counter_name not in globals()):\n",
    "        globals()[counter_name] = sc.accumulator(0)\n",
    "    return globals()[counter_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_counter(752).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_counter(752).add(3)\n",
    "get_counter(752).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's implement a function to estimate the number of unique elements seen since the counters were initialized. This function will combine estimates obtained in each of <b>n</b> hash function, using the Flajolet-Martin method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 30 \n",
    "\n",
    "def print_current_estimate():\n",
    "    '''\n",
    "    Prints the estimate of unique elements seen since counters initialization\n",
    "    '''\n",
    "    estimates = [ 2**(get_counter(i).value) for i in range(n) ]\n",
    "    \n",
    "    from statistics import median, mean\n",
    "    medians = []\n",
    "    values = []\n",
    "    for i, x in enumerate(estimates):\n",
    "        values += [ x ]\n",
    "        if i % 10 == 9:\n",
    "            medians += [ median(values) ]\n",
    "            values = []\n",
    "            \n",
    "    print(f'Estimate: {mean(medians)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_current_estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Step 2</b>: updating counter values with numbers of trailing zeros in hashes\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will emulate a family of $n$ independent hash functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_hash(x, n=1):\n",
    "    return hash(str(x) + str(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_hash(\"apple\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_hash(\"apple\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `bin`function converts integers to their binary representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bin(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will need to count the number of trailing zeros in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def count_trailing_zeros(x):\n",
    "    r = 0\n",
    "    for i in range(len(x)):\n",
    "        if int(x[-(i+1)]) == 0:\n",
    "            r += 1\n",
    "        else:\n",
    "            break\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count_trailing_zeros('1000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def trailing_zeros_in_hash(x, i):\n",
    "    return count_trailing_zeros(bin(my_hash(x, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trailing_zeros_in_hash('apple', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bin(my_hash('apple', 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Using the prototype below, write a function that updates the n counters such that counter <cod>i</code> contains \n",
    "    the maximum number of trailing zeros seen in the binarized outputs of the elements in <code>rdd</code> by hash function <code>my_hash(x, i)</code>. The function should also print the updated counter values.\n",
    "    </li>\n",
    "    Tip: use a MapReduce logic, where:\n",
    "    <ul>\n",
    "        <li><code>map</code> produces (i, trailing_zeros_in_hash) pairs</li>\n",
    "        <li><code>reduce</code> is the max function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 30\n",
    "\n",
    "def update_counters(rdd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd : an RDD of hashable elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counters: an RDD with n elements containing updated values of n counters\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "update_counters(sc.parallelize(['apple', 'banana', 'orange'])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's reset the counters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def reset_counters():\n",
    "    for i in range(n):\n",
    "        counter_name = \"counter-\"+str(i)\n",
    "        globals()[counter_name] = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_counters()\n",
    "get_counter(13).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Step 3</b>: use in a data stream\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's define a stream from the list of (non unique) words in 3 text documents from session 1. The true count of unique words in this dataset is <b>721</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "lines = []\n",
    "for filename in [ 'canada.txt' , 'usa.txt', 'mexico.txt' ]:\n",
    "    with open(op.join('data', 'session1', 'docs', filename), 'r') as f:\n",
    "        lines += f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's create a list of RDDs to be included in a queueStream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdds = []\n",
    "for l in lines:\n",
    "    rdds += [ sc.parallelize(l.split(' ')) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Using Spark's DStream API, build a data stream from <code>rdds</code>, and apply function <code>update_counters</code> to each RDD in this stream. Tip: use stream transformation <code>transform</code>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "It is important for hash functions used in Big Data analyses to be cryptographically secure:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x25a2;  False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "It is important for hash functions used in Big Data analyses to be cryptographically secure:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x2611; False\n",
    "</div>\n",
    "\n",
    "A hash function is said to be cryptographically secure when it is computationally infeasible to retrieve the function argument from the function result. While this property is critical to cyber-security applications, it is often not desirable in Big Data applications, as such applications favor lightweight hash functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is straightforward to adapt a data analysis to data streams:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is straightforward to adapt a data analysis to data streams:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x2611; False\n",
    "</div>\n",
    "\n",
    "Many analyses require to access the entire dataset. This is for instance the case of <b>kmeans clustering</b> or classification with <b>decision trees</b>. Stream-compatible adaptations of these algorithms are sometimes far from trivial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Data streams should always be analyzed with stream algorithms:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x25a2; False\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Data streams should always be analyzed with stream algorithms:<br/>\n",
    "    &#x25a2; True\n",
    "    \n",
    "&#x2611; False\n",
    "</div>\n",
    "\n",
    "Stream algorithms are required when the elements of the data stream encountered so far cannot be analyzed as a whole. This is often the case when:\n",
    "<ol>\n",
    "    <li>Processing, memory and/or storage resources are limited. This is for instance the case on <b>connected objects</b>.</li>\n",
    "    <li>Results are required in near real-time</li>\n",
    "</ol>\n",
    "In other cases, it is often possible to store successive versions of the data stream on disk, and to process this data offline using batch jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mini-project: human activity classification from wearable devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Goal</b>: recognize activites from motion data\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <b>Data</b><br/>\n",
    "REALDISP (<a href=\"http://archive.ics.uci.edu/ml/datasets/REALDISP+Activity+Recognition+Dataset\">link</a>), available in <code>data/session4/subject16_ideal.log</code>.<br/><br/>\n",
    "    <b>Input</b><br/>\n",
    "    9 wearable sensors with 3D acceleration, 3D gyroscope, 3D magnetic field orientation, 4D quaternions\n",
    "    <img src=\"images/session4/sensors.png\"/><br/>\n",
    "    <b>Output</b><br/>\n",
    "    1 of 33 activity classes:<br/>\n",
    "A1: Walking<br/>\n",
    "A2: Jogging<br/>\n",
    "A3: Running<br/>\n",
    "A4: Jump up<br/>\n",
    "A5: Jump front and back<br/>\n",
    "...<br/><br/>\n",
    "<a href=\"https://dl.acm.org/doi/abs/10.1145/2370216.2370437?casa_token=dGTbqvHRvF8AAAAA:Je-UwL8fV_iBLme62kNbeKiPfejHh0qBVTb61Ncm204F0ja2Ceb53Sp3wQ5lQBjhkoICT4kDYK0VaA\">[Banos et al, 2012]</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Method</b>: offline training, online classification\n",
    "    </div>\n",
    "    <br/>\n",
    "We will use data from a single subject \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/session4/diagram.png\" alt=\"diagram\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Let's first have a look at the dataset\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! head -n 2 data/session4//subject16_ideal.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Records are tab-separated\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Let's load the data and look at the class distribution\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'data/session4/subject16_ideal.log'\n",
    "\n",
    "# read dataset\n",
    "with open(filename, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# split and parse to float\n",
    "records = [ [float(val) for val in line.split('\\t')] for line in lines ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "records[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist([record[-1] for record in records], bins=34, label='activity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Dataset is strongly <b>imbalanced</b>\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    We will group records in <b>windows of 50 records</b>\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq = 50  # in Hz, also number of elements in 1-s windows\n",
    "windows = []\n",
    "record = []\n",
    "for i, vals in enumerate(records):\n",
    "    record += [ vals[2:8] + [ vals[119] ]  ] # keep 6 parameters + class\n",
    "    if (i + 1) % freq == 0: # create a new window\n",
    "        windows += [ record ]\n",
    "        record = []\n",
    "\n",
    "# sanity check\n",
    "len(windows[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([record[-1] for window in windows for record in window], bins=34, label='activity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The split in train/test set is done on the <b>time windows</b> (not the individual records).\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(windows)\n",
    "split = round(len(windows)*0.7)\n",
    "train_windows, test_windows = windows[:split], windows[split:]\n",
    "len(train_windows), len(test_windows), len(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([record[-1] for window in train_windows for record in window], bins=34, label='activity - TRAIN')\n",
    "plt.hist([record[-1] for window in test_windows for record in window], bins=34, label='activity - TEST')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "Class distributions are similar in the train and test set</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Offline training\n",
    "\n",
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    As features, we use the <b>mean</b> and <b>standard deviation</b> in each time window</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    The label of a time window is the <b>most frequent label</b> in the time window</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mode, mean, stdev\n",
    "def compute_features_and_class(window):\n",
    "    return    (  [ mean([ float(x[i]) for x in window ]) for i in range(6) ]\n",
    "               + [ stdev([ float(x[i]) for x in window ]) for i in range(6) ]\n",
    "               + [ mode( [int(x[6]) for x in window] ) ])\n",
    "compute_features_and_class(train_windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's iterate on all the features\n",
    "train_features = [ compute_features_and_class(window) for window in train_windows ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "_, (ax1, ax2) = plt.subplots(1,2, figsize=(20, 6))\n",
    "\n",
    "ax1.hist([record[-1] for window in train_windows for record in window], bins=34, label='activity - TRAIN windows')\n",
    "ax1.set_ylabel('Occurrences')\n",
    "ax1.set_xlabel('Activity class')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist([c[-1] for c in train_features], bins=34, color='orange', label='activity - TRAIN features')\n",
    "ax2.set_ylabel('Occurrences')\n",
    "ax2.set_xlabel('Activity class')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    We will train a <b>Random Forest</b> on the dataset, using Spark's MLlib (see session 3)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Starting from <code>train_features</code>, load the data in a DataFrame where each column represent a feature, and the true label is in the last column</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Following the example detailed in Session 3, use a <code>VectorAssembler</code> to create a column named <code>features</code> containing the features to be used by the classifier</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Train a Random Forest from this DataFrame</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Offline classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    As a sanity check, we will compute the offline accuracy of this Random Forest</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, let's compute the features of the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_features = [ compute_features_and_class(window) for window in test_windows ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Convert this list of features to a Spark DataFrame</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Build and run an MLlib pipeline to predict class labels from the Random Forest model:</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Compute the accuracy of this model:</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>\n",
    "    This is not too bad. On this dataset, F1 score is around 0.8 when all the subjects are used (see for instance <a href=\"https://www.mdpi.com/1424-8220/19/22/5026/htm\">here</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Online classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    Let's simulate a data stream using Spark DStream's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.queueStream\">queueStream</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# We will need a Spark context\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A queueStream requires a list of RDDs\n",
    "rdds = []\n",
    "record = []\n",
    "for window in test_windows:\n",
    "    rdds += [ sc.parallelize(window) ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# The stream will pull from the queue every second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "stream = ssc.queueStream(rdds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    <b>Feature computation</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Write a function with the following prototype to compute the features of a time window. Tip: use the following expansion of the variance expression: \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "$$\n",
    "Var(X) = E[X^2]-E[X]^2,\n",
    "$$\n",
    "\n",
    "where $E[X]$ is the average of the values in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "from statistics import mode\n",
    "\n",
    "def features_and_class_rdd(window_rdd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    window_rdd : an RDD containing 50 elements, where each element is a list of length 7 formated as follows:\n",
    "                     [acc_x, acc_y, acc_z, gyr_x, gyr_y, gyr_z, label]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats: an RDD with 1 element, containing the window features and class label, formatted as follows:\n",
    "                    [ mean_acc_x, mean_acc_y, mean_acc_z, mean_gyr_x, mean_gyr_y, mean_gyr_z,\n",
    "                      std_acc_x, std_acc_y, std_acc_z, std_gyr_x, std_gyr_y, std_gyr_z,\n",
    "                      most_freq_label]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "features_and_class_rdd(rdds[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_features_and_class(rdds[0].collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<center>\n",
    "    The difference comes from <b>catastrophic cancellations</b> in the variance estimation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Transform the data stream to compute the features of the time window. Tip: apply <code>features_and_class_rdd</code> to the stream using function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.transform\"><code>transform</code></a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<center>\n",
    "    <b>Prediction</b> and <b>evaluation</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's define two counters to store the number of correct and total predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def correct_counter(context):\n",
    "    if (\"correct_predictions\" not in globals()):\n",
    "        globals()[\"correct_predictions\"] = context.accumulator(0)\n",
    "    return globals()[\"correct_predictions\"]\n",
    "\n",
    "def total_counter(context):\n",
    "    if (\"total_predictions\" not in globals()):\n",
    "        globals()[\"total_predictions\"] = context.accumulator(0)\n",
    "    return globals()[\"total_predictions\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "correct_counter(sc).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Using the following prototype, write a function that applies the Random Forest model to a 1-element RDD representing a time window.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_evaluate(features):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    features: an RDD with 1 element, containing the window features and class label, formatted as follows:\n",
    "                    [ mean_acc_x, mean_acc_y, mean_acc_z, mean_gyr_x, mean_gyr_y, mean_gyr_z,\n",
    "                      std_acc_x, std_acc_y, std_acc_z, std_gyr_x, std_gyr_y, std_gyr_z,\n",
    "                      most_freq_label]\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: an RDD containing the true class label and prediction:\n",
    "                    [ true_class_label, prediction ]\n",
    "    \n",
    "    Prints\n",
    "    ------\n",
    "    Updates counters correct_predictions and total_predictions. Print the current accuracy.\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Use function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.transform\"><code>transform</code></A> to apply <code>predict_and_evaluate</code> to the data stream</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Start the Stream context and watch the evolution of the accuracy!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>\n",
    "    The accuracy should converge to the offline accuracy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul style=\"list-style-image: url('images/do.png');\">\n",
    "    <li>Stop the Stream context</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>\n",
    "    </center>\n",
    "    &#x2611; Practice with a <b>real cluster</b>\n",
    "</div>\n",
    "Interacted with HDFS, processed HDFS file in Spark, submitted Spark job to YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>\n",
    "    </center>\n",
    "    &#x2611; Get familiar with the concept of <b>data streams</b><br/>\n",
    "    &#x2611; Practice with Spark's <b>DStream</b> API<br/>\n",
    "\n",
    "</div>\n",
    "Implemented a counting algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>\n",
    "    </center>\n",
    "    &#x2611; Analyze a data stream for <b>Human Activity Recognition</b> in the Internet of Things\n",
    "</div>\n",
    "\n",
    "Applied windowing, feature extraction, offline training, online classification"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
